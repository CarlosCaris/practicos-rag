{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flujo completo RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "import re\n",
    "from typing import List, Dict\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_qdrant import FastEmbedSparse, RetrievalMode\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_qdrant import RetrievalMode\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader, UnstructuredFileLoader\n",
    "from PyPDF2 import PdfReader\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_all_documents(directory_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Carga documentos PDF desde una carpeta y devuelve una lista de páginas como texto.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Ruta de la carpeta que contiene los archivos PDF.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Lista de textos extraídos de cada página de todos los PDFs en la carpeta.\n",
    "    \"\"\"\n",
    "    all_texts = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.lower().endswith('.pdf'):  # Filtrar solo archivos PDF.\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            print(f\"Cargando archivo: {file_path}\")\n",
    "            reader = PdfReader(file_path)\n",
    "            for page in reader.pages:\n",
    "                all_texts.append(page.extract_text())  # Agregar texto de cada página a la lista\n",
    "    return all_texts\n",
    "\n",
    "\n",
    "\n",
    "def clean_text_and_exclude_sections(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpia el texto eliminando espacios redundantes y caracteres especiales al principio o al final.\n",
    "    Args:\n",
    "        text (str): Texto extraído del PDF.\n",
    "    Returns:\n",
    "        str: Texto limpio.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    # Reemplazar múltiples espacios consecutivos con un único espacio\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Eliminar espacios al inicio y final del texto\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Función para dividir texto en oraciones\n",
    "def split_text_into_sentences(text: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Divide un texto en oraciones basado en '.', '?', y '!' y devuelve una lista de diccionarios.\n",
    "    Args:\n",
    "        text (str): El texto a dividir.\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: Lista de diccionarios con 'sentence' y 'index'.\n",
    "    \"\"\"\n",
    "    single_sentences_list = re.split(r'(?<=[.?!])\\s+', text.strip())\n",
    "    sentences = [{'sentence': sentence, 'index': i} for i, sentence in enumerate(single_sentences_list)]\n",
    "    return sentences\n",
    "\n",
    "# Función para combinar oraciones\n",
    "def combine_sentences(sentences: List[Dict[str, str]], buffer_size: int = 1) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Combina oraciones de acuerdo al tamaño del buffer definido.\n",
    "    Args:\n",
    "        sentences (List[Dict[str, str]]): Lista de oraciones con índices.\n",
    "        buffer_size (int): Número de oraciones antes y después a combinar.\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: Lista con oraciones combinadas.\n",
    "    \"\"\"\n",
    "    for i in range(len(sentences)):\n",
    "        combined_sentence = ''\n",
    "\n",
    "        # Añadir oraciones previas\n",
    "        for j in range(i - buffer_size, i):\n",
    "            if j >= 0:\n",
    "                combined_sentence += sentences[j]['sentence'] + ' '\n",
    "\n",
    "        # Añadir oración actual\n",
    "        combined_sentence += sentences[i]['sentence']\n",
    "\n",
    "        # Añadir oraciones posteriores\n",
    "        for j in range(i + 1, i + 1 + buffer_size):\n",
    "            if j < len(sentences):\n",
    "                combined_sentence += ' ' + sentences[j]['sentence']\n",
    "\n",
    "        # Guardar la oración combinada en el dict actual\n",
    "        sentences[i]['combined_sentence'] = combined_sentence.strip()\n",
    "\n",
    "    return sentences\n",
    "\n",
    "# Función para calcular distancias coseno\n",
    "def calculate_cosine_distances(sentences: List[Dict[str, str]], model_name: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Calcula las distancias coseno entre embeddings de oraciones combinadas.\n",
    "\n",
    "    Args:\n",
    "        sentences (List[Dict[str, Any]]): Lista de oraciones con embeddings combinados.\n",
    "        model_name (str): Nombre del modelo de embeddings.\n",
    "\n",
    "    Returns:\n",
    "        List[float]: Distancias coseno entre embeddings consecutivos.\n",
    "    \"\"\"\n",
    "    # Crear embeddings\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    embeddings = embedding_model.embed_documents([sentence['combined_sentence'] for sentence in sentences])\n",
    "\n",
    "    # Añadir embeddings a las oraciones\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence['embedding'] = embeddings[i]\n",
    "\n",
    "    distances = []\n",
    "    for i in range(len(sentences) - 1):\n",
    "        sim = cosine_similarity([sentences[i]['embedding']], [sentences[i + 1]['embedding']])[0][0]\n",
    "        distances.append(1 - sim)\n",
    "\n",
    "    return distances\n",
    "\n",
    "# Función para dividir en fragmentos\n",
    "def split_into_chunks(sentences: List[Dict[str, str]], distances: List[float], threshold: float) -> List[str]:\n",
    "    \"\"\"\n",
    "    Divide el texto en fragmentos basado en la distancia coseno entre oraciones.\n",
    "\n",
    "    Args:\n",
    "        sentences (List[Dict[str, str]]): Lista de oraciones.\n",
    "        distances (List[float]): Distancias entre oraciones consecutivas.\n",
    "        threshold (float): Umbral para decidir la separación de fragmentos.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Lista de fragmentos de texto.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start_index = 0\n",
    "\n",
    "    for i, distance in enumerate(distances):\n",
    "        if distance > threshold:\n",
    "            chunk = ' '.join(sentence['sentence'] for sentence in sentences[start_index:i + 1])\n",
    "            chunks.append(chunk)\n",
    "            start_index = i + 1\n",
    "\n",
    "    if start_index < len(sentences):\n",
    "        chunk = ' '.join(sentence['sentence'] for sentence in sentences[start_index:])\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Embeddings\n",
    "\n",
    "def create_qdrant_store(model_name: str, chunks: List[str]) -> QdrantVectorStore:\n",
    "    \"\"\"\n",
    "    Crea y devuelve un QdrantVectorStore a partir de un modelo de embeddings y una lista de chunks de texto.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Nombre del modelo de embeddings.\n",
    "        chunks (List[str]): Lista de fragmentos de texto.\n",
    "\n",
    "    Returns:\n",
    "        QdrantVectorStore: Objeto de almacenamiento Qdrant.\n",
    "    \"\"\"\n",
    "    # Crear embeddings con el modelo especificado\n",
    "    open_source_embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "    # Preparar documentos para Qdrant\n",
    "    documents_for_qdrant = [\n",
    "        Document(page_content=chunk, metadata={\"chunk_index\": i})\n",
    "        for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "\n",
    "    # Crear la tienda de vectores en memoria\n",
    "    qdrant = QdrantVectorStore.from_documents(\n",
    "        documents_for_qdrant,\n",
    "        embedding=open_source_embeddings,\n",
    "        location=\":memory:\",  # Puedes cambiar la ubicación para persistencia\n",
    "        collection_name=\"my_documents\",\n",
    "        retrieval_mode=RetrievalMode.DENSE,\n",
    "    )\n",
    "    \n",
    "    return qdrant \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_rag_chain(model, openai_api_key, qdrant):\n",
    "    \"\"\"\n",
    "    Crea una cadena RAG (Retrieval-Augmented Generation) usando LangChain.\n",
    "    \n",
    "    :param model: Nombre del modelo OpenAI (str).\n",
    "    :param openai_api_key: Clave de la API de OpenAI (str).\n",
    "    :param qdrant: Objeto QdrantVectorStore configurado como un retriever.\n",
    "    :return: Objeto rag_chain.\n",
    "    \"\"\"\n",
    "    # Configurar el modelo OpenAI\n",
    "    llm = ChatOpenAI(\n",
    "        model=model,\n",
    "        temperature=0.7,  # Ajusta la creatividad según sea necesario\n",
    "        openai_api_key=openai_api_key\n",
    "    )\n",
    "\n",
    "    # Descargar y configurar el prompt desde LangChain Hub\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # Función para formatear los documentos\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Configurar el retriever desde Qdrant\n",
    "    retriever = qdrant.as_retriever()\n",
    "\n",
    "    # Crear la cadena RAG\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return rag_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../practicos-rag/otros/usa2/CFR-2024-vol8.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# Umbral para dividir chunks\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 1. Cargar texto de los documentos PDF\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m pdf_texts \u001b[38;5;241m=\u001b[39m \u001b[43mload_pdf_all_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 2. Combinar texto de todas las páginas en un solo string\u001b[39;00m\n\u001b[1;32m     14\u001b[0m full_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(pdf_texts)\n",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m, in \u001b[0;36mload_pdf_all_documents\u001b[0;34m(directory_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mCarga documentos PDF desde una carpeta y devuelve una lista de páginas como texto.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    List[str]: Lista de textos extraídos de cada página de todos los PDFs en la carpeta.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m all_texts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# Filtrar solo archivos PDF.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m         file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory_path, filename)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../practicos-rag/otros/usa2/CFR-2024-vol8.pdf'"
     ]
    }
   ],
   "source": [
    "# Parámetros\n",
    "model = \"gpt-3.5-turbo\"\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model_name = \"sentence-transformers/paraphrase-MiniLM-L6-v2\"\n",
    "#directory_path = \"../practicos-rag/data/USA/\"\n",
    "directory_path = \"/Users/v0a02bg/practicos-rag/otros/usa2/CFR-2024-vol8.pdf\"\n",
    "buffer_size = 2  # Número de oraciones antes y después a combinar\n",
    "threshold = 0.5  # Umbral para dividir chunks\n",
    "\n",
    "# 1. Cargar texto de los documentos PDF\n",
    "pdf_texts = load_pdf_all_documents(directory_path)\n",
    "\n",
    "# 2. Combinar texto de todas las páginas en un solo string\n",
    "full_text = \" \".join(pdf_texts)\n",
    "\n",
    "# 3. Limpiar texto y excluir secciones no deseadas\n",
    "cleaned_text = clean_text_and_exclude_sections(full_text)\n",
    "\n",
    "# 4. Procesar el texto para dividirlo en chunks\n",
    "# 4.1 Dividir texto en oraciones\n",
    "sentences = split_text_into_sentences(cleaned_text)\n",
    "\n",
    "# 4.2 Combinar oraciones con un buffer\n",
    "combined_sentences = combine_sentences(sentences, buffer_size)\n",
    "\n",
    "# 4.3 Calcular distancias coseno entre oraciones combinadas\n",
    "distances = calculate_cosine_distances(combined_sentences, model_name)\n",
    "\n",
    "# 4.4 Dividir texto en chunks basados en el umbral\n",
    "chunks = split_into_chunks(combined_sentences, distances, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " flujo_completo3.ipynb\n",
      "01_intro.ipynb\n",
      "02_chunking.ipynb\n",
      "03_embedding.ipynb\n",
      "04_vector_databases.ipynb\n",
      "CRM-Q2-FY25-Earnings-Press-Release-w-financials.pdf\n",
      "Prueba2.ipynb\n",
      "\u001b[34mUSA\u001b[m\u001b[m/\n",
      "\u001b[34mcacao\u001b[m\u001b[m/\n",
      "chunk.py\n",
      "data.txt\n",
      "document_splitter.py\n",
      "embedding.py\n",
      "example.md\n",
      "flujo_completo.ipynb\n",
      "flujo_completo2.ipynb\n",
      "load.py\n",
      "main.ipynb\n",
      "main_app.py\n",
      "mit.txt\n",
      "note.txt\n",
      "\u001b[34musa2\u001b[m\u001b[m/\n",
      "visual_instruction_tunning.pdf\n",
      "vstore.py\n"
     ]
    }
   ],
   "source": [
    "/Users/v0a02bg/practicos-rag/otros/usa2/CFR-2024-vol8.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando archivo: ../practicos-rag/data/cacao/Regulaciones cacao y chocolate 2003.pdf\n",
      "Respuesta: I don't know.\n"
     ]
    }
   ],
   "source": [
    "# Parámetros\n",
    "model = \"gpt-3.5-turbo\"\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model_name = \"sentence-transformers/paraphrase-MiniLM-L6-v2\"\n",
    "#directory_path = \"../practicos-rag/data/USA/\"\n",
    "directory_path = \"../practicos-rag/data/cacao\"\n",
    "buffer_size = 2  # Número de oraciones antes y después a combinar\n",
    "threshold = 0.5  # Umbral para dividir chunks\n",
    "\n",
    "# 1. Cargar texto de los documentos PDF\n",
    "pdf_texts = load_pdf_all_documents(directory_path)\n",
    "\n",
    "# 2. Combinar texto de todas las páginas en un solo string\n",
    "full_text = \" \".join(pdf_texts)\n",
    "\n",
    "# 3. Limpiar texto y excluir secciones no deseadas\n",
    "cleaned_text = clean_text_and_exclude_sections(full_text)\n",
    "\n",
    "# 4. Procesar el texto para dividirlo en chunks\n",
    "# 4.1 Dividir texto en oraciones\n",
    "sentences = split_text_into_sentences(cleaned_text)\n",
    "\n",
    "# 4.2 Combinar oraciones con un buffer\n",
    "combined_sentences = combine_sentences(sentences, buffer_size)\n",
    "\n",
    "# 4.3 Calcular distancias coseno entre oraciones combinadas\n",
    "distances = calculate_cosine_distances(combined_sentences, model_name)\n",
    "\n",
    "# 4.4 Dividir texto en chunks basados en el umbral\n",
    "chunks = split_into_chunks(combined_sentences, distances, threshold)\n",
    "\n",
    "# 5. Crear el Qdrant store con los chunks procesados\n",
    "qdrant_store = create_qdrant_store(model_name, chunks)\n",
    "\n",
    "# 6. Configurar y crear la cadena RAG\n",
    "rag_chain = create_rag_chain(model, openai_api_key, qdrant_store)\n",
    "\n",
    "# 7. Realizar inferencia con una pregunta específica\n",
    "question = \"What are the mandatory data elements that must be submitted in the Automated Commercial Environment (ACE) for articles regulated by the FDA?\"\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "# 8. Imprimir la respuesta\n",
    "print(\"Respuesta:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_qdrant.qdrant.QdrantVectorStore at 0x7ff64c67d710>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta: The proportions of cocoa butter in chocolate must be calculated according to the weight of the dry matter, with not less than 20 percent cocoa butter. The regulations specify that the chocolate product must contain not less than 18 percent cocoa butter. The total dry cocoa solids content must be not less than 35 percent, including at least 18 percent cocoa butter.\n"
     ]
    }
   ],
   "source": [
    "question = \"Can you tell me what are the proportions of cocoa butter in the chocolate?\"\n",
    "response = rag_chain.invoke(question)\n",
    "print(\"Respuesta:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta: Apart from cocoa butter, vegetable fats such as Illipe, Palm-oil, Sal, Shea, Kokum gurgi, and Mango kernel can be authorized for use in chocolate products according to the regulations. These vegetable fats must comply with specific criteria, including being non-lauric vegetable fats rich in certain types of triglycerides and obtained through specific processes like refining or fractionation. Coconut oil can also be used in chocolate for the manufacture of ice cream and similar frozen products.\n"
     ]
    }
   ],
   "source": [
    "question = \"Which vegetable fats, apart from cocoa butter, are authorized to be used in chocolate products according to the regulations?\"\n",
    "response = rag_chain.invoke(question)\n",
    "print(\"Respuesta:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta: To submit electronic entry of FDA-regulated products to the ACE system according to Section 1.72, the following information must be provided: FDA Country of Production, Complete FDA Product Code consistent with the invoice description, and Full Intended Use Code. Additionally, Importer of record contact information such as telephone and email address must be included in the submission.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from src.retrievers.rag_retriever import create_rag_chain\n",
    "\n",
    "with open('qdrant_store.pkl', 'rb') as f:\n",
    "    qdrant_loaded = pickle.load(f)\n",
    "\n",
    "rag_chain = create_rag_chain(\"gpt-3.5-turbo\", openai_api_key, qdrant_loaded)\n",
    "\n",
    "question = \"What information must be submitted to the ACE system for electronic entry of FDA-regulated products according to Section 1.72?\"\n",
    "response = rag_chain.invoke(question)\n",
    "print(\"Respuesta:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
