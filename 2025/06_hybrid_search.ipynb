{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Search\n",
    "\n",
    "Hybrid search combines traditional keyword-based search with semantic search to provide more accurate and relevant results. In the RAG application, it facilitates the discovery of relevant research articles based on user queries by integrating keyword-based search with semantic search capabilities. This integration enables the application to retrieve articles that match both keywords and semantic meaning, making it particularly useful for handling complex queries involving nuanced concepts, synonyms, and related ideas.\n",
    "\n",
    "![Hybrid Search](images/Hybrid_Search.png)\n",
    "\n",
    "\n",
    "In this notebook, we will delve into the implementation details of the hybrid search approach in the RAG application, exploring how it leverages both keyword-based and semantic search techniques to provide a more effective search experience.\n",
    "\n",
    "Here are the steps:\n",
    "* [Loading chunked dataset](#loading-the-chunks-from-the-previous-steps)\n",
    "* [Sparse Index](#Hybrid-Search---Sparse-Index)\n",
    "* [Dense Index](#hybrid-search---dense-index)\n",
    "* [Merging Results](#hybrid-search---merging-results)\n",
    "* [Generating a reply with merged results](#using-merged-results-to-generate-a-reply)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Search - Sparse Index\n",
    "\n",
    "We will use bm25 supported database to complement the semantic search with the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bm25s\n",
    "from bm25s.tokenization import Tokenizer, Tokenized\n",
    "import Stemmer  # optional: for stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the chunks from the previous steps\n",
    "\n",
    "We will use the chunks from the AI Arxiv dataset, we used before. These chunks were split using semantic chunking and enriched with context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "corpus_json = json.load(open('../data/corpus.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Sparse Index\n",
    "\n",
    "We will use an in-memory index using BM25. Many (vector) databases support BM25 natively, and many others support indexing and searching on calculated sparse vectors.\n",
    "\n",
    "In this example, we will also define a stemmer and stop-words to clean up the text and better select the tokens/terms that will be indexed in the sparse index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_text = [doc[\"text\"] for doc in corpus_json]\n",
    "\n",
    "# optional: create a stemmer\n",
    "english_stemmer = Stemmer.Stemmer(\"english\")\n",
    "\n",
    "# Initialize the Tokenizer with the stemmer\n",
    "sparse_tokenizer = Tokenizer(\n",
    "    stemmer=english_stemmer,\n",
    "    lower=True, # lowercase the tokens\n",
    "    stopwords=\"english\",  # or pass a list of stopwords\n",
    "    splitter=r\"\\w+\",  # by default r\"(?u)\\b\\w\\w+\\b\", can also be a function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 'an', 'and', 'are', 'as', 'at', 'be', 'but', 'by', 'for', 'if', 'in', 'into', 'is', 'it', 'no', 'not', 'of', 'on', 'or', 'such', 'that', 'the', 'their', 'then', 'there', 'these', 'they', 'this', 'to', 'was', 'will', 'with')\n"
     ]
    }
   ],
   "source": [
    "print(sparse_tokenizer.stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    }
   ],
   "source": [
    "# Tokenize the corpus and only keep the ids (faster and saves memory)\n",
    "corpus_sparse_tokens = (\n",
    "    sparse_tokenizer\n",
    "    .tokenize(\n",
    "        corpus_text, \n",
    "        update_vocab=True, # update the vocab as we tokenize\n",
    "        return_as=\"ids\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the BM25 retriever and attach your corpus_json to it\n",
    "sparse_index = bm25s.BM25(corpus=corpus_json)\n",
    "# Now, index the corpus_tokens (the corpus_json is not used yet)\n",
    "sparse_index.index(corpus_sparse_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokenizer vocabulary includes 1690 tokens/terms\n",
      "The index of the context is 128\n"
     ]
    }
   ],
   "source": [
    "vocab_dict = sparse_tokenizer.get_vocab_dict()\n",
    "print(f\"The tokenizer vocabulary includes {len(vocab_dict)} tokens/terms\")\n",
    "\n",
    "focus_token = 'context'\n",
    "focus_token_index = vocab_dict.get(focus_token)\n",
    "print(f\"The index of the {focus_token} is {focus_token_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer can encode (convert the text into ids) and decode (convert the ids back into text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['context']]\n"
     ]
    }
   ],
   "source": [
    "print(sparse_tokenizer.decode([[focus_token_index]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Sparse Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': array([0.73858595, 0.75115275, 1.0805569 , ..., 1.6571839 , 1.6571839 ,\n",
      "       1.6571839 ], shape=(4039,), dtype=float32), 'indices': array([ 0,  9, 10, ..., 45, 45, 45], shape=(4039,), dtype=int32), 'indptr': array([   0,    0,   12, ..., 4037, 4038, 4039],\n",
      "      shape=(1691,), dtype=int32), 'num_docs': 46}\n"
     ]
    }
   ],
   "source": [
    "print(sparse_index.scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each token, the index holds the list of documents (chunks) that include it, and the score of that token in that document (chunk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of the token `context` in the BM25 retriever: 128\n",
      "Document ID Score\n",
      "0 0.4434834\n",
      "2 0.7355118\n",
      "3 0.5371069\n",
      "4 0.90847206\n",
      "13 0.5116058\n",
      "14 0.9670208\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "15: **1.105641484260559**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 0.7444794\n",
      "37 0.6708645\n",
      "41 0.7355118\n"
     ]
    }
   ],
   "source": [
    "token_index = vocab_dict.get(focus_token)\n",
    "print(f\"Index of the token `{focus_token}` in the BM25 retriever: {token_index}\")\n",
    "score_index = sparse_index.scores.get('indptr')[token_index]\n",
    "next_score_index = sparse_index.scores.get('indptr')[token_index+1]\n",
    "\n",
    "\n",
    "print(\"Document ID\", \"Score\")\n",
    "max_score = max(sparse_index.scores['data'][score_index:next_score_index])\n",
    "\n",
    "for i in range(score_index, next_score_index):\n",
    "    doc_id = sparse_index.scores['indices'][i]\n",
    "    doc_score = sparse_index.scores['data'][i]\n",
    "    if doc_score == max_score:\n",
    "        display(Markdown(f\"{doc_id}: **{doc_score}**\"))\n",
    "    else:\n",
    "        print(doc_id, doc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching the Sparse Index\n",
    "\n",
    "As we are doing in the dense index, we need to tokenize and encode the query text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[128, 129, 16]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Query the corpus\n",
    "query = \"What is context size of Mixtral?\"\n",
    "query_tokens = (\n",
    "    sparse_tokenizer\n",
    "    .tokenize(\n",
    "        [query], \n",
    "        update_vocab=False, \n",
    "        return_as=\"ids\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(query_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use the encoded query to search the sparse index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1 (score: 1.99): {'id': 2, 'text': 'expertsâ ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, Mixture of Experts Layer i gating inputs af outputs router expert\\n\\nThis chunk describes the key architectural details of the Mixtral model, a sparse mixture-of-experts language model that outperforms larger models like Llama 2 70B and GPT-3.5 on various benchmarks.', 'metadata': {'title': 'Mixtral of Experts', 'arxiv_id': '2401.04088', 'references': ['1905.07830']}}\n",
      "Rank 2 (score: 1.86): {'id': 14, 'text': \"Active Params French Arc-c HellaS MMLU German Arc-c HellaS MMLU Spanish Arc-c HellaS MMLU Italian Arc-c HellaS MMLU 33B 70B 13B 42.9% 65.4% 49.0% 39.3% 68.1% 49.9% 49.9% 72.5% 64.3% 49.4% 70.9% 65.1% 58.2% 77.4% 70.9% 54.3% 73.0% 71.5% 55.4% 77.6% 72.5% 52.8% 75.1% 70.9% 41.1% 63.3% 48.7% 47.3% 68.7% 64.2% 45.7% 69.8% 52.3% 50.5% 74.5% 66.0% Table 4: Comparison of Mixtral with Llama on Multilingual Benchmarks. On ARC Challenge, Hellaswag, and MMLU, Mixtral outperforms Llama 2 70B on 4 languages: French, German, Spanish, and Italian. # 3.2 Long range performance To assess the capabilities of Mixtral to tackle long context, we evaluate it on the passkey retrieval task introduced in [23], a synthetic task designed to measure the ability of the model to retrieve a passkey inserted randomly in a long prompt. Results in Figure 4 (Left) show that Mixtral achieves a 100% retrieval accuracy regardless of the context length or the position of passkey in the sequence. Figure 4 (Right) shows that the perplexity of Mixtral on a subset of the proof-pile dataset [2] decreases monotonically as the size of the context increases. Passkey Performance ry 0.8 0.6 04 0.2 0.0 OK 4K 8K 12K 16K 20K 24K 28K Seq Len Passkey Loc\\n\\nThe chunk discusses Mixtral's performance on multilingual benchmarks and its ability to handle long-range context, demonstrating its strong capabilities in these areas.\", 'metadata': {'title': 'Mixtral of Experts', 'arxiv_id': '2401.04088', 'references': ['1905.07830']}}\n",
      "Rank 3 (score: 1.46): {'id': 1, 'text': 'chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license. Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/ # Introduction In this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes. Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the â\\n\\nThis chunk introduces Mixtral 8x7B, a sparse mixture of experts language model that outperforms Llama 2 70B and GPT-3.5 on most benchmarks. It describes the key architectural details of Mixtral, including its use of a sparse mixture-of-experts network, and mentions that the base and instruct models are released under the Apache 2.0 license.', 'metadata': {'title': 'Mixtral of Experts', 'arxiv_id': '2401.04088', 'references': ['1905.07830']}}\n",
      "Rank 4 (score: 1.33): {'id': 15, 'text': \"3.8 â Mixtral_8x7B 3.5 32 > $3.0 i] 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length Passkey Performance ry 3.8 â Mixtral_8x7B 3.5 0.8 32 > 0.6 $3.0 i] 228 04 fos 0.2 a 2.0 0.0 OK 4K 8K 12K 16K 20K 24K 28K 0 5k 10k 15k 20k 25k 30k Seq Len Context length Figure 4: Long range performance of Mixtral. (Left) Mixtral has 100% retrieval accuracy of the Passkey task regardless of the location of the passkey and length of the input sequence. (Right) The perplexity of Mixtral on the proof-pile dataset decreases monotonically as the context length increases.\\n\\nThe chunk discusses the long-range performance of the Mixtral model, demonstrating its ability to retrieve a passkey regardless of its location in a long input sequence, and showing that the model's perplexity on the proof-pile dataset decreases as the context length increases.\", 'metadata': {'title': 'Mixtral of Experts', 'arxiv_id': '2401.04088', 'references': ['1905.07830']}}\n",
      "Rank 5 (score: 1.32): {'id': 0, 'text': '4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â\\n\\nThis chunk introduces Mixtral 8x7B, a sparse mixture of experts language model that outperforms Llama 2 70B and GPT-3.5 on various benchmarks. It also describes the model architecture and the fine-tuned Mixtral 8x7B - Instruct model.', 'metadata': {'title': 'Mixtral of Experts', 'arxiv_id': '2401.04088', 'references': ['1905.07830']}}\n",
      "Rank 6 (score: 1.24): {'id': 12, 'text': 'Size and Efficiency. We compare our performance to the Llama 2 family, aiming to understand Mixtral modelsâ efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture- of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active parameters, Mixtral is able to outperform Llama 2 70B across most categories. Note that this analysis focuses on the active parameter count (see Section 2.1), which is directly proportional to the inference compute cost, but does not consider the memory costs and hardware utilization. The memory costs for serving Mixtral are proportional to its sparse parameter count, 47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer introduces additional overhead due to the routing mechanism and due to the increased memory loads when running more than one expert per device. They are more suitable for batched workloads where one can reach a good degree of arithmetic intensity. Comparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of Mixtral 8x7B compared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the two other models. On MMLU, Mixtral obtains a better performance, despite its significantly smaller capacity (47B tokens compared to 70B). For MT Bench, we report the performance of the latest GPT-3.5-Turbo model available, gpt-3.5-turbo-1106. 2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\\n\\nThis chunk discusses the size and efficiency of the Mixtral model, comparing its performance to the Llama 2 family of models. It highlights that Mixtral, as a sparse mixture-of-experts model, uses significantly fewer active parameters than Llama 2 70B while outperforming it across most benchmarks. The chunk also compares the performance of Mixtral 8x7B to Llama 2 70B and GPT-3.5.', 'metadata': {'title': 'Mixtral of Experts', 'arxiv_id': '2401.04088', 'references': ['1905.07830']}}\n",
      "Rank 7 (score: 1.12): {'id': 4, 'text': \"Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud. # 2 Architectural details Mixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture parameters are summarized in Table 1.\\n\\nThis chunk describes the architectural details of the Mixtral language model, including its use of a transformer architecture with a 32k token context length and mixture-of-expert layers. It also mentions the model's open-source licensing and deployment options.\", 'metadata': {'title': 'Mixtral of Experts', 'arxiv_id': '2401.04088', 'references': ['1905.07830']}}\n",
      "Rank 8 (score: 0.89): {'id': 41, 'text': 'Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. [33] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023.\\n\\nThe chunk discusses two references related to language model benchmarking, including the Hellaswag dataset and the MT-Bench and Chatbot Arena benchmarks. This is situated within the broader context of the paper, which introduces the Mixtral language model and evaluates its performance on various benchmarks.', 'metadata': {'title': 'Mixtral of Experts', 'arxiv_id': '2401.04088', 'references': ['1905.07830']}}\n",
      "Rank 9 (score: 0.80): {'id': 3, 'text': 'Figure 1: Mixture of Experts Layer. Each input vector is assigned to 2 of the 8 experts by a router. The layerâ s output is the weighted sum of the outputs of the two selected experts. In Mixtral, an expert is a standard feedforward block as in a vanilla transformer architecture. Mixtral demonstrates superior capabilities in mathematics, code generation, and tasks that require multilingual understanding, significantly outperforming Llama 2 70B in these domains. Experiments show that Mixtral is able to successfully retrieve information from its context window of 32k tokens, regardless of the sequence length and the location of the information in the sequence. We also present Mixtral 8x7B â Instruct, a chat model fine-tuned to follow instructions using supervised fine-tuning and Direct Preference Optimization [25]. Its performance notably surpasses that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human evaluation benchmarks. Mixtral â Instruct also demonstrates reduced biases, and a more balanced sentiment profile in benchmarks such as BBQ, and BOLD. We release both Mixtral 8x7B and Mixtral 8x7B â\\n\\nThis chunk describes the Mixture of Experts layer architecture used in the Mixtral model, as well as the superior performance of Mixtral compared to other models on various benchmarks, including mathematics, code generation, and multilingual tasks. It also introduces the Mixtral 8x7B - Instruct model, which is fine-tuned to follow instructions and outperforms other chat models on human evaluation benchmarks.', 'metadata': {'title': 'Mixtral of Experts', 'arxiv_id': '2401.04088', 'references': ['1905.07830']}}\n",
      "Rank 10 (score: 0.75): {'id': 13, 'text': \"4 LLaMA 2 70B GPT-3.5 MMLU (MCQ in 57 subjects) 69.9% 70.0% 70.6% HellaSwag (10-shot) 87.1% 85.5% 86.7% ARC Challenge (25-shot) 85.1% 85.2% 85.8% WinoGrande (5-shot) 83.2% 81.6% 81.2% MBPP (pass@1) 49.8% 52.2% 60.7% GSM-8K (5-shot) 53.6% 57.1% 58.4% MT Bench (for Instruct Models) 6.86 8.32 8.30 # Mixtral 8x7B Table 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5. Mixtral outperforms or matches Llama 2 70B and GPT-3.5 performance on most metrics. Evaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts. # 3.1 Multilingual benchmarks Compared to Mistral 7B, we significantly upsample the proportion of multilingual data during pretraining. The extra capacity allows Mixtral to perform well on multilingual benchmarks while maintaining a high accuracy in English. In particular, Mixtral significantly outperforms Llama 2 70B in French, German, Spanish, and Italian, as shown in Table 4.\\n\\nThis chunk presents a comparison of the performance of Mixtral 8x7B, Llama 2 70B, and GPT-3.5 on various benchmarks, as well as an analysis of Mixtral's performance on multilingual benchmarks.\", 'metadata': {'title': 'Mixtral of Experts', 'arxiv_id': '2401.04088', 'references': ['1905.07830']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Query the corpus\n",
    "sparse_results, sparse_scores = sparse_index.retrieve(query_tokens, k=10)\n",
    "\n",
    "for i in range(sparse_results.shape[1]):\n",
    "    doc, score = sparse_results[0, i], sparse_scores[0, i]\n",
    "    print(f\"Rank {i+1} (score: {score:.2f}): {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Search - Dense Index\n",
    "\n",
    "For the Hybrid Search, we also need the dense index using the vector database, as we used in the previous steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creaing the Dense Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "qdrant_client = QdrantClient(\n",
    "    \":memory:\"\n",
    ") \n",
    "\n",
    "# Create the embedding encoder\n",
    "dense_encoder = SentenceTransformer('all-MiniLM-L6-v2') # Model to create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "collection_name = \"hybrid_search\"\n",
    "\n",
    "dense_index = qdrant_client.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "        vectors_config=models.VectorParams(\n",
    "        size=dense_encoder.get_sentence_embedding_dimension(), # Vector size is defined by used model\n",
    "        distance=models.Distance.COSINE\n",
    "    )\n",
    ")\n",
    "print(dense_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize!\n",
    "qdrant_client.upload_points(\n",
    "    collection_name=collection_name,\n",
    "    points=[\n",
    "        models.PointStruct(\n",
    "            id=idx,\n",
    "            vector=dense_encoder.encode(doc[\"text\"]).tolist(),\n",
    "            payload=doc\n",
    "        ) for idx, doc in enumerate(corpus_json) # data is the variable holding all the enriched texts\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching the Dense Index\n",
    "\n",
    "We will start with encoding the query with the dense encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = dense_encoder.encode(query).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use the encoded query to search the dense index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_results = qdrant_client.search(\n",
    "    collection_name=collection_name,\n",
    "    query_vector=query_vector,\n",
    "    limit=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id:  15\n",
      "text:  3.8 â Mixtral_8x7B 3.5 32 > $3.0 i] 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length Passkey Performance ry 3.8 â Mixtral_8x7B 3.5 0.8 32 > 0.6 $3.0 i] 228 04 fos 0.2 a 2.0 0.0 OK 4K 8K 12K 16K 20K 24K 28K 0 5k 10k 15k 20k 25k 30k Seq Len Context length Figure 4: Long range performance of Mixtral. (Left) Mixtral has 100% retrieval accuracy of the Passkey task regardless of the location of the passkey and length of the input sequence. (Right) The perplexity of Mixtral on the proof-pile dataset decreases monotonically as the context length increases.\n",
      "\n",
      "The chunk discusses the long-range performance of the Mixtral model, demonstrating its ability to retrieve a passkey regardless of its location in a long input sequence, and showing that the model's perplexity on the proof-pile dataset decreases as the context length increases.\n",
      "dense_score:  0.618097593406871\n",
      "sparse_score:  0.618097593406871\n",
      "--------------------------------\n",
      "id:  4\n",
      "text:  Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud. # 2 Architectural details Mixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture parameters are summarized in Table 1.\n",
      "\n",
      "This chunk describes the architectural details of the Mixtral language model, including its use of a transformer architecture with a 32k token context length and mixture-of-expert layers. It also mentions the model's open-source licensing and deployment options.\n",
      "dense_score:  0.49721748274575195\n",
      "sparse_score:  0.49721748274575195\n",
      "--------------------------------\n",
      "id:  2\n",
      "text:  expertsâ ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, Mixture of Experts Layer i gating inputs af outputs router expert\n",
      "\n",
      "This chunk describes the key architectural details of the Mixtral model, a sparse mixture-of-experts language model that outperforms larger models like Llama 2 70B and GPT-3.5 on various benchmarks.\n",
      "dense_score:  0.4423914075704028\n",
      "sparse_score:  0.4423914075704028\n",
      "--------------------------------\n",
      "id:  6\n",
      "text:  Table 1: Model architecture. # j nâ G(x)i Â· Ei(x). i=0 Here, G(x)i denotes the n-dimensional output of the gating network for the i-th expert, and Ei(x) is the output of the i-th expert network. If the gating vector is sparse, we can avoid computing the outputs of experts whose gates are zero. There are multiple alternative ways of implementing G(x) [6, 15, 35], but a simple and performant one is implemented by taking the softmax over the Top-K logits of a linear layer [28].\n",
      "\n",
      "The chunk describes the architectural details of the Mixtral model, specifically the Sparse Mixture of Experts (SMoE) layer that is used in the model.\n",
      "dense_score:  0.44049945702391397\n",
      "sparse_score:  0.44049945702391397\n",
      "--------------------------------\n",
      "id:  7\n",
      "text:  We use G(x) := Softmax(TopK(x Â· Wg)), where (TopK(â ))i := â i if â i is among the top-K coordinates of logits â â Rn and (TopK(â ))i := â â otherwise. The value of K â the number of experts used per token â is a hyper-parameter that modu- lates the amount of compute used to process each token. If one increases n while keeping K fixed, one # 1https://mistral.ai/news/mixtral-of-experts/\n",
      "\n",
      "This chunk describes the gating mechanism used in the Mixture of Experts (MoE) layer of the Mixtral model. It explains how the router network selects the top-K experts to process each token, and how this allows the model to increase its parameter count while keeping the computational cost constant.\n",
      "dense_score:  0.4245351360334579\n",
      "sparse_score:  0.4245351360334579\n",
      "--------------------------------\n",
      "id:  42\n",
      "text:  10 [34] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023. [35] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, Quoc V Le, James Laudon, et al.\n",
      "\n",
      "The chunk contains references to related work on evaluating foundation models, which is relevant to the overall topic of the document discussing the Mixtral language model.\n",
      "dense_score:  0.3848539697547131\n",
      "sparse_score:  0.3848539697547131\n",
      "--------------------------------\n",
      "id:  45\n",
      "text:  e ArXiv â eâ DM Mathematics â e Github â eâ Gutenberg â eâ PhilPapers â eâ PubMed â e- StackExchange â e-â Wikipedia (en) # Abstracts Figure 10: Repeated consecutive assignments per MoE layer. Repeated assignments occur a lot more often than they would with uniform assignments (materialized by the dashed lines). Patterns are similar across datasets with less repetitions for DM Mathematics. 13\n",
      "\n",
      "This chunk discusses the analysis of expert assignment patterns in the Mixtral model, showing that there is a high degree of temporal locality in the expert selection, especially at higher layers of the model. This analysis provides insights into the behavior of the sparse mixture-of-experts architecture used in Mixtral.\n",
      "dense_score:  0.38037543483425895\n",
      "sparse_score:  0.38037543483425895\n",
      "--------------------------------\n",
      "id:  0\n",
      "text:  4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â\n",
      "\n",
      "This chunk introduces Mixtral 8x7B, a sparse mixture of experts language model that outperforms Llama 2 70B and GPT-3.5 on various benchmarks. It also describes the model architecture and the fine-tuned Mixtral 8x7B - Instruct model.\n",
      "dense_score:  0.3677009632109649\n",
      "sparse_score:  0.3677009632109649\n",
      "--------------------------------\n",
      "id:  5\n",
      "text:  Parameter Value dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size num_experts top_k_experts # 2.1 Sparse Mixture of Experts We present a brief overview of the Mixture of Experts layer (Figure 1). For a more in-depth overview, see [12]. The output of the MoE module for a given input x is determined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating networkâ s output. i.e. given n expert networks {E0, Ei, ..., Enâ 1}, the output of the expert layer is given by:\n",
      "\n",
      "This chunk describes the architectural details of the Mixtral model, specifically the Sparse Mixture of Experts layer that is a key component of the model.\n",
      "dense_score:  0.36711107128737175\n",
      "sparse_score:  0.36711107128737175\n",
      "--------------------------------\n",
      "id:  11\n",
      "text:  Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â = Mistral Â° 20 â e LlaMA2 78 (138 348 70B 7B (138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params Figure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math. Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a superior performance in code and mathematics benchmarks.\n",
      "\n",
      "This chunk presents a comparison of the performance of the Mixtral 8x7B and Mistral 7B models against the Llama 2 family of models across various benchmarks, including commonsense reasoning, world knowledge, reading comprehension, math, and code generation. It highlights that Mixtral outperforms Llama 2 70B on most metrics while using significantly fewer active parameters.\n",
      "dense_score:  0.3540554206829311\n",
      "sparse_score:  0.3540554206829311\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "for result in dense_results:\n",
    "    print(\"id: \", result.id)\n",
    "    print(\"text: \", result.payload[\"text\"])\n",
    "    print(\"dense_score: \", result.score)\n",
    "    print(\"sparse_score: \", result.score)\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Search - Merging Results\n",
    "\n",
    "There are a few options to merge the results from the two methods (sparse and dense). In this notebook, we will use a simple weighted average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_with_scores = []\n",
    "for hit in dense_results:\n",
    "    doc_id = hit.payload[\"id\"]\n",
    "    doc_text = next((doc for doc in corpus_json if doc[\"id\"] == doc_id), None)[\"text\"]\n",
    "    doc_dense_score = hit.score\n",
    "    documents_with_scores.append({\n",
    "        \"id\": doc_id,\n",
    "        \"text\": doc_text,\n",
    "        \"dense_score\": doc_dense_score\n",
    "    })\n",
    "\n",
    "for i, result in enumerate(sparse_results[0]):\n",
    "    doc_id = result[\"id\"]\n",
    "    doc_text = next((doc for doc in corpus_json if doc[\"id\"] == doc_id), None)[\"text\"]\n",
    "    doc_sparse_score = sparse_scores[0][i]\n",
    "    for doc in documents_with_scores:\n",
    "        if doc[\"id\"] == doc_id:\n",
    "            doc[\"sparse_score\"] = doc_sparse_score\n",
    "            break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 11,\n",
       " 'text': 'Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â = Mistral Â° 20 â e LlaMA2 78 (138 348 70B 7B (138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params Figure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math. Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a superior performance in code and mathematics benchmarks.\\n\\nThis chunk presents a comparison of the performance of the Mixtral 8x7B and Mistral 7B models against the Llama 2 family of models across various benchmarks, including commonsense reasoning, world knowledge, reading comprehension, math, and code generation. It highlights that Mixtral outperforms Llama 2 70B on most metrics while using significantly fewer active parameters.',\n",
       " 'dense_score': 0.3540554206829311}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_with_scores[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents_with_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id:  15\n",
      "text:  3.8 â Mixtral_8x7B 3.5 32 > $3.0 i] 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length Passkey Performance ry 3.8 â Mixtral_8x7B 3.5 0.8 32 > 0.6 $3.0 i] 228 04 fos 0.2 a 2.0 0.0 OK 4K 8K 12K 16K 20K 24K 28K 0 5k 10k 15k 20k 25k 30k Seq Len Context length Figure 4: Long range performance of Mixtral. (Left) Mixtral has 100% retrieval accuracy of the Passkey task regardless of the location of the passkey and length of the input sequence. (Right) The perplexity of Mixtral on the proof-pile dataset decreases monotonically as the context length increases.\n",
      "\n",
      "The chunk discusses the long-range performance of the Mixtral model, demonstrating its ability to retrieve a passkey regardless of its location in a long input sequence, and showing that the model's perplexity on the proof-pile dataset decreases as the context length increases.\n",
      "dense_score:  0.618097593406871\n",
      "sparse_score:  1.333729\n",
      "--------------------------------\n",
      "id:  4\n",
      "text:  Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud. # 2 Architectural details Mixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture parameters are summarized in Table 1.\n",
      "\n",
      "This chunk describes the architectural details of the Mixtral language model, including its use of a transformer architecture with a 32k token context length and mixture-of-expert layers. It also mentions the model's open-source licensing and deployment options.\n",
      "dense_score:  0.49721748274575195\n",
      "sparse_score:  1.1242076\n",
      "--------------------------------\n",
      "id:  2\n",
      "text:  expertsâ ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, Mixture of Experts Layer i gating inputs af outputs router expert\n",
      "\n",
      "This chunk describes the key architectural details of the Mixtral model, a sparse mixture-of-experts language model that outperforms larger models like Llama 2 70B and GPT-3.5 on various benchmarks.\n",
      "dense_score:  0.4423914075704028\n",
      "sparse_score:  1.9919167\n",
      "--------------------------------\n",
      "id:  6\n",
      "text:  Table 1: Model architecture. # j nâ G(x)i Â· Ei(x). i=0 Here, G(x)i denotes the n-dimensional output of the gating network for the i-th expert, and Ei(x) is the output of the i-th expert network. If the gating vector is sparse, we can avoid computing the outputs of experts whose gates are zero. There are multiple alternative ways of implementing G(x) [6, 15, 35], but a simple and performant one is implemented by taking the softmax over the Top-K logits of a linear layer [28].\n",
      "\n",
      "The chunk describes the architectural details of the Mixtral model, specifically the Sparse Mixture of Experts (SMoE) layer that is used in the model.\n",
      "dense_score:  0.44049945702391397\n",
      "--------------------------------\n",
      "id:  7\n",
      "text:  We use G(x) := Softmax(TopK(x Â· Wg)), where (TopK(â ))i := â i if â i is among the top-K coordinates of logits â â Rn and (TopK(â ))i := â â otherwise. The value of K â the number of experts used per token â is a hyper-parameter that modu- lates the amount of compute used to process each token. If one increases n while keeping K fixed, one # 1https://mistral.ai/news/mixtral-of-experts/\n",
      "\n",
      "This chunk describes the gating mechanism used in the Mixture of Experts (MoE) layer of the Mixtral model. It explains how the router network selects the top-K experts to process each token, and how this allows the model to increase its parameter count while keeping the computational cost constant.\n",
      "dense_score:  0.4245351360334579\n",
      "--------------------------------\n",
      "id:  42\n",
      "text:  10 [34] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023. [35] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, Quoc V Le, James Laudon, et al.\n",
      "\n",
      "The chunk contains references to related work on evaluating foundation models, which is relevant to the overall topic of the document discussing the Mixtral language model.\n",
      "dense_score:  0.3848539697547131\n",
      "--------------------------------\n",
      "id:  45\n",
      "text:  e ArXiv â eâ DM Mathematics â e Github â eâ Gutenberg â eâ PhilPapers â eâ PubMed â e- StackExchange â e-â Wikipedia (en) # Abstracts Figure 10: Repeated consecutive assignments per MoE layer. Repeated assignments occur a lot more often than they would with uniform assignments (materialized by the dashed lines). Patterns are similar across datasets with less repetitions for DM Mathematics. 13\n",
      "\n",
      "This chunk discusses the analysis of expert assignment patterns in the Mixtral model, showing that there is a high degree of temporal locality in the expert selection, especially at higher layers of the model. This analysis provides insights into the behavior of the sparse mixture-of-experts architecture used in Mixtral.\n",
      "dense_score:  0.38037543483425895\n",
      "--------------------------------\n",
      "id:  0\n",
      "text:  4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â\n",
      "\n",
      "This chunk introduces Mixtral 8x7B, a sparse mixture of experts language model that outperforms Llama 2 70B and GPT-3.5 on various benchmarks. It also describes the model architecture and the fine-tuned Mixtral 8x7B - Instruct model.\n",
      "dense_score:  0.3677009632109649\n",
      "sparse_score:  1.3166082\n",
      "--------------------------------\n",
      "id:  5\n",
      "text:  Parameter Value dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size num_experts top_k_experts # 2.1 Sparse Mixture of Experts We present a brief overview of the Mixture of Experts layer (Figure 1). For a more in-depth overview, see [12]. The output of the MoE module for a given input x is determined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating networkâ s output. i.e. given n expert networks {E0, Ei, ..., Enâ 1}, the output of the expert layer is given by:\n",
      "\n",
      "This chunk describes the architectural details of the Mixtral model, specifically the Sparse Mixture of Experts layer that is a key component of the model.\n",
      "dense_score:  0.36711107128737175\n",
      "--------------------------------\n",
      "id:  11\n",
      "text:  Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â = Mistral Â° 20 â e LlaMA2 78 (138 348 70B 7B (138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params Figure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math. Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a superior performance in code and mathematics benchmarks.\n",
      "\n",
      "This chunk presents a comparison of the performance of the Mixtral 8x7B and Mistral 7B models against the Llama 2 family of models across various benchmarks, including commonsense reasoning, world knowledge, reading comprehension, math, and code generation. It highlights that Mixtral outperforms Llama 2 70B on most metrics while using significantly fewer active parameters.\n",
      "dense_score:  0.3540554206829311\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(documents_with_scores)):\n",
    "    print(\"id: \", documents_with_scores[i][\"id\"])\n",
    "    print(\"text: \", documents_with_scores[i][\"text\"])\n",
    "    print(\"dense_score: \", documents_with_scores[i][\"dense_score\"])\n",
    "    try:\n",
    "        print(\"sparse_score: \", documents_with_scores[i][\"sparse_score\"])\n",
    "    except:\n",
    "        pass\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will normalize the scores of each index, and than calculate a weighted score that gives more weight (0.8) to the dense index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Normalize the two types of scores\n",
    "dense_scores = np.array([doc.get(\"dense_score\", 0) for doc in documents_with_scores])\n",
    "sparse_scores = np.array([doc.get(\"sparse_score\", 0) for doc in documents_with_scores])\n",
    "\n",
    "dense_scores_normalized = (dense_scores - np.min(dense_scores)) / (np.max(dense_scores) - np.min(dense_scores))\n",
    "sparse_scores_normalized = (sparse_scores - np.min(sparse_scores)) / (np.max(sparse_scores) - np.min(sparse_scores))\n",
    "\n",
    "# Calculate a weighted score with alpha of 0.2 to the sparse score\n",
    "alpha = 0.2\n",
    "weighted_scores = (1 - alpha) * dense_scores_normalized + alpha * sparse_scores_normalized\n",
    "\n",
    "# Pick up the top 3 documents with the weighted score\n",
    "top_docs = sorted(\n",
    "    zip(\n",
    "        documents_with_scores, \n",
    "        weighted_scores\n",
    "    ), \n",
    "    key=lambda x: x[1], \n",
    "    reverse=True\n",
    ")[:3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'id': 15,\n",
       "   'text': \"3.8 â Mixtral_8x7B 3.5 32 > $3.0 i] 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length Passkey Performance ry 3.8 â Mixtral_8x7B 3.5 0.8 32 > 0.6 $3.0 i] 228 04 fos 0.2 a 2.0 0.0 OK 4K 8K 12K 16K 20K 24K 28K 0 5k 10k 15k 20k 25k 30k Seq Len Context length Figure 4: Long range performance of Mixtral. (Left) Mixtral has 100% retrieval accuracy of the Passkey task regardless of the location of the passkey and length of the input sequence. (Right) The perplexity of Mixtral on the proof-pile dataset decreases monotonically as the context length increases.\\n\\nThe chunk discusses the long-range performance of the Mixtral model, demonstrating its ability to retrieve a passkey regardless of its location in a long input sequence, and showing that the model's perplexity on the proof-pile dataset decreases as the context length increases.\",\n",
       "   'dense_score': 0.618097593406871,\n",
       "   'sparse_score': np.float32(1.333729)},\n",
       "  np.float64(0.933914139866595)),\n",
       " ({'id': 4,\n",
       "   'text': \"Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud. # 2 Architectural details Mixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture parameters are summarized in Table 1.\\n\\nThis chunk describes the architectural details of the Mixtral language model, including its use of a transformer architecture with a 32k token context length and mixture-of-expert layers. It also mentions the model's open-source licensing and deployment options.\",\n",
       "   'dense_score': 0.49721748274575195,\n",
       "   'sparse_score': np.float32(1.1242076)},\n",
       "  np.float64(0.5466321134659456)),\n",
       " ({'id': 2,\n",
       "   'text': 'expertsâ ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, Mixture of Experts Layer i gating inputs af outputs router expert\\n\\nThis chunk describes the key architectural details of the Mixtral model, a sparse mixture-of-experts language model that outperforms larger models like Llama 2 70B and GPT-3.5 on various benchmarks.',\n",
       "   'dense_score': 0.4423914075704028,\n",
       "   'sparse_score': np.float32(1.9919167)},\n",
       "  np.float64(0.46764205422541594))]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using merged results to generate a reply\n",
    "\n",
    "We can now take the merged results and call the LLM to generate the reply to the user's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a variable to hold the search results for the generation model\n",
    "search_results = [doc[0]['text'] for doc in top_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now time to connect to the large language model\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "model_name = \"gpt-4.1-mini-2025-04-14\"\n",
    "completion = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are chatbot, an research expert. Your top priority is to help guide users to understand reserach papers.\"},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "        {\"role\": \"assistant\", \"content\": str(search_results)}\n",
    "    ]\n",
    ")\n",
    "\n",
    "response_text = str(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hybrid search reply to  What is context size of Mixtral? : The context size of Mixtral is 32,000 tokens (32k tokens).\n"
     ]
    }
   ],
   "source": [
    "print(\"hybrid search reply to \", query,\":\", response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the retrieved documents to be used in the next reranking notebook, which demonstrates a more advanced method to merge Hybrid Search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../data/dense_results.json', 'w') as f:\n",
    "    json.dump([dense_result.payload for dense_result in dense_results], f, default=str)\n",
    "\n",
    "with open('../data/sparse_results.json', 'w') as f:\n",
    "    json.dump([sparse_result for sparse_result in sparse_results[0]], f, default=str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
