{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flujo completo RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "import re\n",
    "from typing import List, Dict\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_qdrant import FastEmbedSparse, RetrievalMode\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_qdrant import RetrievalMode\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Script Limpieza y Segmentación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para cargar documentos PDF \n",
    "def load_pdf_documents(file_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Carga documentos PDF y devuelve una lista de páginas como texto.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Ruta del archivo PDF a cargar.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Lista de textos extraídos de cada página del PDF.\n",
    "    \"\"\"\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load()\n",
    "    return [doc.page_content for doc in docs]  # Extrae texto como lista de strings\n",
    "\n",
    "def load_pdf_all_documents(directory_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Carga documentos PDF desde una carpeta y devuelve una lista de páginas como texto.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Ruta de la carpeta que contiene los archivos PDF.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Lista de textos extraídos de cada página de todos los PDFs en la carpeta.\n",
    "    \"\"\"\n",
    "    all_texts = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.lower().endswith('.pdf'):  # Filtrar solo archivos PDF.\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            print(f\"Cargando archivo: {file_path}\")\n",
    "            reader = PdfReader(file_path)\n",
    "            for page in reader.pages:\n",
    "                all_texts.append(page.extract_text())  # Agregar texto de cada página a la lista\n",
    "    return all_texts\n",
    "\n",
    "# Función para limpiar el texto y excluir secciones específicas\n",
    "def clean_text_and_exclude_sections(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpia el texto extraído de un PDF, excluyendo índices y bibliografías \n",
    "    basados en patrones comunes.\n",
    "\n",
    "    Args:\n",
    "        text (str): Texto extraído del PDF.\n",
    "\n",
    "    Returns:\n",
    "        str: Texto limpio y sin secciones de índice o bibliografía.\n",
    "    \"\"\"\n",
    "    # Reemplazar saltos de línea por espacios\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "    \n",
    "    # Eliminar espacios múltiples\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Excluir secciones específicas (Índice, Referencias, Bibliografía)\n",
    "    text = re.sub(r'(?i)(Índice|Table of Contents).*?(Referencias|Bibliografía|References)', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Opcional: Detectar bibliografías al final del documento\n",
    "    text = re.sub(r'(?i)(Referencias|Bibliografía|References).*$', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Eliminar caracteres no deseados (mantener alfanuméricos y signos de puntuación básicos)\n",
    "    text = re.sub(r'[^a-zA-Z0-9.,;?!:()\\s]', '', text)\n",
    "    \n",
    "    # Corrección de palabras divididas por guiones al final de línea (ej., \"pro-\\nject\" -> \"project\")\n",
    "    text = re.sub(r'-\\s', '', text)\n",
    "    \n",
    "    # Eliminar espacios al inicio y final\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Función para dividir texto en oraciones\n",
    "def split_text_into_sentences(text: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Divide un texto en oraciones basado en '.', '?', y '!' y devuelve una lista de diccionarios.\n",
    "\n",
    "    Args:\n",
    "        text (str): El texto a dividir.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: Lista de diccionarios con 'sentence' y 'index'.\n",
    "    \"\"\"\n",
    "    single_sentences_list = re.split(r'(?<=[.?!])\\s+', text.strip())\n",
    "    sentences = [{'sentence': sentence, 'index': i} for i, sentence in enumerate(single_sentences_list)]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Función para combinar oraciones\n",
    "def combine_sentences(sentences: List[Dict[str, str]], buffer_size: int = 1) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Combina oraciones de acuerdo al tamaño del buffer definido.\n",
    "\n",
    "    Args:\n",
    "        sentences (List[Dict[str, str]]): Lista de oraciones con índices.\n",
    "        buffer_size (int): Número de oraciones antes y después a combinar.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: Lista con oraciones combinadas.\n",
    "    \"\"\"\n",
    "    for i in range(len(sentences)):\n",
    "        combined_sentence = ''\n",
    "\n",
    "        # Añadir oraciones previas\n",
    "        for j in range(i - buffer_size, i):\n",
    "            if j >= 0:\n",
    "                combined_sentence += sentences[j]['sentence'] + ' '\n",
    "\n",
    "        # Añadir oración actual\n",
    "        combined_sentence += sentences[i]['sentence']\n",
    "\n",
    "        # Añadir oraciones posteriores\n",
    "        for j in range(i + 1, i + 1 + buffer_size):\n",
    "            if j < len(sentences):\n",
    "                combined_sentence += ' ' + sentences[j]['sentence']\n",
    "\n",
    "        # Guardar la oración combinada en el dict actual\n",
    "        sentences[i]['combined_sentence'] = combined_sentence.strip()\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Embeddings \n",
    "\n",
    "def create_qdrant_store(model_name, combined_sentences):\n",
    "    \"\"\"\n",
    "    Crea y devuelve un QdrantVectorStore a partir de un modelo de embeddings y una lista de frases combinadas.\n",
    "    \n",
    "    :param model_name: Nombre del modelo de embeddings (str).\n",
    "    :param combined_sentences: Lista de diccionarios con las claves 'combined_sentence' y 'index' (list).\n",
    "    :return: Objeto QdrantVectorStore.\n",
    "    \"\"\"\n",
    "    # Crear embeddings con el modelo especificado\n",
    "    open_source_embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "    # Preparar documentos para Qdrant\n",
    "    documents_for_qdrant = [\n",
    "        Document(page_content=doc[\"combined_sentence\"], metadata={\"index\": doc[\"index\"]})\n",
    "        for doc in combined_sentences\n",
    "    ]\n",
    "\n",
    "    # Crear la tienda de vectores en memoria\n",
    "    qdrant = QdrantVectorStore.from_documents(\n",
    "        documents_for_qdrant,\n",
    "        embedding=open_source_embeddings,\n",
    "        location=\":memory:\",  # Puedes cambiar la ubicación si necesitas persistencia\n",
    "        collection_name=\"my_documents\",\n",
    "        retrieval_mode=RetrievalMode.DENSE,\n",
    "    )\n",
    "    \n",
    "    return qdrant\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_rag_chain(model, openai_api_key, qdrant):\n",
    "    \"\"\"\n",
    "    Crea una cadena RAG (Retrieval-Augmented Generation) usando LangChain.\n",
    "    \n",
    "    :param model: Nombre del modelo OpenAI (str).\n",
    "    :param openai_api_key: Clave de la API de OpenAI (str).\n",
    "    :param qdrant: Objeto QdrantVectorStore configurado como un retriever.\n",
    "    :return: Objeto rag_chain.\n",
    "    \"\"\"\n",
    "    # Configurar el modelo OpenAI\n",
    "    llm = ChatOpenAI(\n",
    "        model=model,\n",
    "        temperature=0.7,  # Ajusta la creatividad según sea necesario\n",
    "        openai_api_key=openai_api_key\n",
    "    )\n",
    "\n",
    "    # Descargar y configurar el prompt desde LangChain Hub\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # Función para formatear los documentos\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Configurar el retriever desde Qdrant\n",
    "    retriever = qdrant.as_retriever()\n",
    "\n",
    "    # Crear la cadena RAG\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return rag_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando archivo: ../practicos-rag/data/USA/CFR-2024-vol8.pdf\n",
      "Cargando archivo: ../practicos-rag/data/USA/CFR-2024-vol3.pdf\n",
      "Cargando archivo: ../practicos-rag/data/USA/CFR-2024-vol2.pdf\n",
      "Cargando archivo: ../practicos-rag/data/USA/CFR-2024-vol1.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The mandatory data elements that must be submitted in the Automated Commercial Environment (ACE) for articles regulated by the FDA include Pasteurization, method of bacterial count, authority to sample and inspect, and scoring. These elements are essential for compliance with regulations under the Federal Import Milk Act.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Parametros\n",
    "model = \"gpt-3.5-turbo\"\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model_name = \"sentence-transformers/paraphrase-MiniLM-L6-v2\"\n",
    "directory_path = \"../practicos-rag/data/USA/\"\n",
    "buffer_size = 2  # Número de oraciones antes y después a combina\n",
    "\n",
    "## Funciones\n",
    "# Cargar texto del PDF\n",
    "pdf_texts = load_pdf_all_documents(directory_path)\n",
    "# Combinar texto de todas las páginas\n",
    "full_text = \" \".join(pdf_texts)\n",
    "\n",
    "# Limpiar texto\n",
    "cleaned_text = clean_text_and_exclude_sections(full_text)\n",
    "\n",
    "# Dividir en oraciones\n",
    "sentences = split_text_into_sentences(cleaned_text)\n",
    "\n",
    "# Combinar oraciones\n",
    "combined_sentences = combine_sentences(sentences, buffer_size)\n",
    "# Crear el Qdrant store\n",
    "qdrant_store = create_qdrant_store(model_name, combined_sentences)\n",
    "\n",
    "# Crear la cadena RAG\n",
    "rag_chain = create_rag_chain(model, openai_api_key, qdrant_store)\n",
    "\n",
    "#Inferencia \n",
    "rag_chain.invoke(\"What are the mandatory data elements that must be submitted in the Automated Commercial Environment (ACE) for articles regulated by the FDA?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
