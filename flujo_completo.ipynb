{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flujo completo de un modelo de RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "import re\n",
    "from typing import List, Dict\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Script Limpieza y Segmentación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para cargar documentos PDF\n",
    "def load_pdf_documents(file_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Carga documentos PDF y devuelve una lista de páginas como texto.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Ruta del archivo PDF a cargar.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Lista de textos extraídos de cada página del PDF.\n",
    "    \"\"\"\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load()\n",
    "    return [doc.page_content for doc in docs]  # Extrae texto como lista de strings\n",
    "\n",
    "\n",
    "# Función para limpiar el texto y excluir secciones específicas\n",
    "def clean_text_and_exclude_sections(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpia el texto extraído de un PDF, excluyendo índices y bibliografías \n",
    "    basados en patrones comunes.\n",
    "\n",
    "    Args:\n",
    "        text (str): Texto extraído del PDF.\n",
    "\n",
    "    Returns:\n",
    "        str: Texto limpio y sin secciones de índice o bibliografía.\n",
    "    \"\"\"\n",
    "    # Reemplazar saltos de línea por espacios\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "    \n",
    "    # Eliminar espacios múltiples\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Excluir secciones específicas (Índice, Referencias, Bibliografía)\n",
    "    text = re.sub(r'(?i)(Índice|Table of Contents).*?(Referencias|Bibliografía|References)', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Opcional: Detectar bibliografías al final del documento\n",
    "    text = re.sub(r'(?i)(Referencias|Bibliografía|References).*$', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Eliminar caracteres no deseados (mantener alfanuméricos y signos de puntuación básicos)\n",
    "    text = re.sub(r'[^a-zA-Z0-9.,;?!:()\\s]', '', text)\n",
    "    \n",
    "    # Corrección de palabras divididas por guiones al final de línea (ej., \"pro-\\nject\" -> \"project\")\n",
    "    text = re.sub(r'-\\s', '', text)\n",
    "    \n",
    "    # Eliminar espacios al inicio y final\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Función para dividir texto en oraciones\n",
    "def split_text_into_sentences(text: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Divide un texto en oraciones basado en '.', '?', y '!' y devuelve una lista de diccionarios.\n",
    "\n",
    "    Args:\n",
    "        text (str): El texto a dividir.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: Lista de diccionarios con 'sentence' y 'index'.\n",
    "    \"\"\"\n",
    "    single_sentences_list = re.split(r'(?<=[.?!])\\s+', text.strip())\n",
    "    sentences = [{'sentence': sentence, 'index': i} for i, sentence in enumerate(single_sentences_list)]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Función para combinar oraciones\n",
    "def combine_sentences(sentences: List[Dict[str, str]], buffer_size: int = 1) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Combina oraciones de acuerdo al tamaño del buffer definido.\n",
    "\n",
    "    Args:\n",
    "        sentences (List[Dict[str, str]]): Lista de oraciones con índices.\n",
    "        buffer_size (int): Número de oraciones antes y después a combinar.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: Lista con oraciones combinadas.\n",
    "    \"\"\"\n",
    "    for i in range(len(sentences)):\n",
    "        combined_sentence = ''\n",
    "\n",
    "        # Añadir oraciones previas\n",
    "        for j in range(i - buffer_size, i):\n",
    "            if j >= 0:\n",
    "                combined_sentence += sentences[j]['sentence'] + ' '\n",
    "\n",
    "        # Añadir oración actual\n",
    "        combined_sentence += sentences[i]['sentence']\n",
    "\n",
    "        # Añadir oraciones posteriores\n",
    "        for j in range(i + 1, i + 1 + buffer_size):\n",
    "            if j < len(sentences):\n",
    "                combined_sentence += ' ' + sentences[j]['sentence']\n",
    "\n",
    "        # Guardar la oración combinada en el dict actual\n",
    "        sentences[i]['combined_sentence'] = combined_sentence.strip()\n",
    "\n",
    "    return sentences\n",
    "\n",
    "file_path = \"../practicos-rag/data/USA/CFR-2024-vol1.pdf\"\n",
    "buffer_size = 2  # Número de oraciones antes y después a combinar\n",
    "# Cargar texto del PDF\n",
    "pdf_docs = load_pdf_documents(file_path)\n",
    "\n",
    "# Combinar texto de todas las páginas\n",
    "full_text = \" \".join(pdf_docs)\n",
    "\n",
    "# Limpiar texto\n",
    "cleaned_text = clean_text_and_exclude_sections(full_text)\n",
    "\n",
    "# Dividir en oraciones\n",
    "sentences = split_text_into_sentences(cleaned_text)\n",
    "\n",
    "# Combinar oraciones\n",
    "combined_sentences = combine_sentences(sentences, buffer_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_for_qdrant = [doc[\"combined_sentence\"] for doc in combined_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: 1.382 What labeling or marking require ments apply to a detained article of food? 1.383 What expedited procedures apply when FDA initiates a seizure action against a detained perishable food? 1.384 When does a detention order termi nate? HOW DOES FDA ORDER A DETENTION? 1.391 Who approves a detention order?, Metadata: {'index': 75, '_id': '918e0990c5be4c568a17ae195a4e4d02', '_collection_name': 'my_documents'}\n",
      "Content: 1.392 Who receives a copy of the detention order? 1.393 What information must FDA include in the detention order? WHAT IS THE APPEAL PROCESS FOR A DETENTION ORDER? 1.401 Who is entitled to appeal? 1.402 What are the requirements for submit ting an appeal?, Metadata: {'index': 80, '_id': 'd5e4d42ed45448a3be827b4765b3c6e5', '_collection_name': 'my_documents'}\n",
      "Content: 1.380 Where and under what conditions must the detained article of food be held? 1.381 May a detained article of food be de livered to another entity or transferred to another location? 1.382 What labeling or marking require ments apply to a detained article of food? 1.383 What expedited procedures apply when FDA initiates a seizure action against a detained perishable food? 1.384 When does a detention order termi nate?, Metadata: {'index': 73, '_id': '95b2100ba3bc4847a836977649938ee3', '_collection_name': 'my_documents'}\n",
      "Content: 1.391 Who approves a detention order? 1.392 Who receives a copy of the detention order? 1.393 What information must FDA include in the detention order? WHAT IS THE APPEAL PROCESS FOR A DETENTION ORDER? 1.401 Who is entitled to appeal?, Metadata: {'index': 79, '_id': 'fddbc1a865a94655b85ad6b1831494c6', '_collection_name': 'my_documents'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_qdrant import QdrantVectorStore, RetrievalMode\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Crear embeddings\n",
    "open_source_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "# Preparar documentos para Qdrant\n",
    "documents_for_qdrant = [\n",
    "    Document(page_content=doc[\"combined_sentence\"], metadata={\"index\": doc[\"index\"]})\n",
    "    for doc in combined_sentences\n",
    "]\n",
    "\n",
    "# Crear la tienda de vectores\n",
    "qdrant = QdrantVectorStore.from_documents(\n",
    "    documents_for_qdrant,\n",
    "    embedding=open_source_embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"my_documents\",\n",
    "    retrieval_mode=RetrievalMode.DENSE,\n",
    ")\n",
    "\n",
    "# Consulta\n",
    "query = \"What did the president say about Ketanji Brown Jackson?\"\n",
    "found_docs = qdrant.similarity_search(query)\n",
    "\n",
    "# Imprimir resultados\n",
    "for doc in found_docs:\n",
    "    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
